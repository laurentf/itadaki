normalisation /255 au lieu de preprocess de efficientnet

ProblÃ¨me archi initial => calcul d'une distance euclidienne sur des embeddings dÃ©jÃ  L2-normalisÃ©s, ce qui est redondant. 
Pour des embeddings normalisÃ©s, il faut utiliser plutÃ´t la similaritÃ© cosinus.

  # L2 normalization for embeddings
    def l2_normalize(x):
        return tf.nn.l2_normalize(x, axis=1)
    
    embeddings = Lambda(l2_normalize, name='l2_normalize')(x)

Loss inadaptÃ©e => constrastive loss initialement mais cela n'est pas fait pour des embeddings dÃ©jÃ  normalisÃ©s
Il faut plutÃ´t utiliser une triplet loss ou cosine similarity loss

Erreur => augmentation activÃ©e sur donnÃ©es de validation :(

Pas de cache sur les images preprocess

Batch size 16/32 ?

Moins de couche sur la tÃªte custom (Ã§a suffit pour tester le transfer learning, le plus important reste le fine tuning)

Custom Lambda Ã  la place des custom Layer

Sauvegarde du triplet model au lieu de l'embedding model...

Souci score sim <1 pour image de la base
ğŸ¯ PrÃ©diction
Je parie que :
Soit l'image n'est pas dans la base (path diffÃ©rent)
Soit il y a un bug dans search_similar_recipes